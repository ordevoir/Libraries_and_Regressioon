{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit Learn <a href=\"https://scikit-learn.org/stable/#\"><img id='logo' height=36 src=\"https://scikit-learn.org/stable/_static/scikit-learn-logo-small.png\"></a>\n",
    "<style>\n",
    "#logo {\n",
    "    background-color: white;\n",
    "    border-radius: 10px;\n",
    "    }\n",
    "</style>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn (или sklearn) это библиотека на языке программирования Python, которая используется для решения задач машинного обучения, в том числе классификации, регрессии, кластеризации и обработки данных. Она предоставляет реализацию многих алгоритмов машинного обучения, таких как метод опорных векторов (SVM), случайный лес (Random Forest), метод главных компонент (PCA) и многие другие.\n",
    "\n",
    "Библиотека scikit-learn позволяет быстро создавать и применять модели машинного обучения на больших наборах данных. Она также включает в себя множество функций для предобработки данных, оценки качества моделей и выбора оптимальных параметров моделей."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка и визуализация данных "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_data = pd.read_csv('ice_cream_selling_train.csv')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data['temperature'].to_numpy()\n",
    "y_train = train_data['ice_cream_sales'].to_numpy()\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train, y_train)\n",
    "plt.ylim(0, 270)\n",
    "plt.xlabel('temperature')\n",
    "plt.ylabel('ice_cream_sales')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выбор модели и обучение "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()       # создается объект модели\n",
    "# обучение модели на данных\n",
    "model.fit(X_train.reshape(-1, 1), y_train)      "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Прогнозирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузка тестовых данных и конвертация в массивы NumPy\n",
    "test_data = pd.read_csv('ice_cream_selling_test.csv')\n",
    "X_test = test_data['temperature'].to_numpy()\n",
    "y_test = test_data['ice_cream_sales'].to_numpy()\n",
    "\n",
    "# произведем прогнозирование на тестовых данных\n",
    "y_test_predicted = model.predict(X_test.reshape(-1, 1))\n",
    "\n",
    "header = f\"X_test\\ty_test\\ty_test_predicted\"\n",
    "print(header, '\\n', '-'*len(header), sep='')\n",
    "for j in range(10):\n",
    "    i = np.random.randint(0, len(y_test))\n",
    "    print(f\"{X_test[i]}\\t{y_test[i]}\\t{int(y_test_predicted[i])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# произведем прогнозирование на тренировочных данных\n",
    "y_train_predicted = model.predict(X_train.reshape(-1, 1))\n",
    "\n",
    "plt.scatter(X_train, y_train, c='grey', s=15)\n",
    "plt.scatter(X_test, y_test, c='green')\n",
    "plt.plot(X_train, y_train_predicted)\n",
    "plt.ylim(0, 270)\n",
    "\n",
    "plt.xlabel('temperature')\n",
    "plt.ylabel('ice_cream_sales')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оценка результата"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы оценить регрессионную модель, можно оценить среднеквадратическое отклонение, используюя функцию `mean_squared_error()` из модуля `metrics` библиотеки `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "train_rmse = mse(y_train, y_train_predicted, squared=False)\n",
    "test_rmse = mse(y_test, y_test_predicted, squared=False)\n",
    "\n",
    "print(f'{train_rmse = }, {test_rmse = }')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Среднеквадратическая ошибка**\n",
    "$$\n",
    "\\mathrm{MSE} = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y_i} )^2 =  \\frac{1}{n} \\left[(y_1 - \\hat{y_1})^2 + \\dots + (y_n - \\hat{y_n} )^2 \\right]\n",
    "$$\n",
    "где $y_i$ - значения из тестовой выборки, $\\hat y_i$ - значения, спрогнозированные моделью (для $i$-го образца.\n",
    "\n",
    "**Среднеквадратическое отклонение** выражается как корень от среднеквадратической ошибки:\n",
    "$$\n",
    "\\mathrm{RMSE} = \\sqrt{\\mathrm{MSE}}\n",
    "$$\n",
    "Функция `mean_squared_error()` вычисляет среднеквадратическую ошибку для двух массивов. Если задать параметр `squared=False`, то функция вычислит среднеквадратическое отклонение. Для наглядности, вычислим среднеквадратическую ошибку средствами NumPy, чтобы лучше представить себе, что именно происходит внутри функции `mean_squared_error()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mse = np.mean((y_test - y_test_predicted) ** 2)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "print(test_rmse)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Что под капотом?\n",
    "Модель в машинном обучении всегда содержит внутри себя параметры, которые подгоняются к оптимальным значениям в процессе обучения. Мы выбрали модель линейной регрессии, которая реализована в классе `LinearRegression` модуля `linear_model` библиотеки `sklearn`. В результате обучения этой модели на одномерных данных, модель создает прямую $f(x) = a + bx$, которая выражает зависимость целевого признака (target feature) от единственного (нецелевого) признака. В нашем случае эта прямая должна отражать зависимость продаж мороженного (ice_cream_sales) от уличной температуры (temperature). Параметрами модели являются коэффициенты $a$ и $b$, которые однозначно задают прямую на плоскости признаков.\n",
    "\n",
    "В случае задачи одномерной линейной регрессии \"подогнать параметры к оптимальным значениям\" означает найти такие $a$ и $b$, при которых прямая $f(x) = a + bx$ лучше всего выражает зависимость продаж мороженного от температуры. Но как мы будем определять, какая прямая \"лучше\"? Рассмотрим пару вариантов. Значение температуры произвольной $i$-строки в данных обозначим $x_i$, а соответствующее ему значение продаж мороженного обозначим $y_i$. Когда модель будет прогнозировать значение продаж мороженного по заданному значению температуры $x_i$, результатом будет $\\hat y_i = a + bx_i$. Присмотримся к разности $y_i - \\hat y_i$ (которую в литературе называют невязкой). Если точка оказывается выше прямой, то это значит, что $y_i > \\hat y_i$ и разность будет положительной. Если точка ниже прямой, то разность отрицательная, а если точка лежит на прямой, то разность равна нулю. Что если мы возьмем среднее разностей по всем образцам наших данных и будем считать, что лучшая прямая та, у которой минимальна средняя разность?\n",
    "$$\n",
    "\\frac{1}{n}\\left[(y_1 - \\hat y_1) + \\dots + (y_n - \\hat y_n) \\right]\n",
    "$$\n",
    "Рассмотрим три прямые и соответствующие им средние разности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_differences(y1, y2):\n",
    "    return np.sum(y1 - y2) / len(y1)\n",
    "\n",
    "def f(x, a, b):\n",
    "    return a + b*x\n",
    "\n",
    "y_cross = f(X_train, 349.164, -9)\n",
    "y_over = f(X_train, 200, 3)\n",
    "\n",
    "print(\"mean differences\")\n",
    "print(f\"green: {mean_differences(y_train, y_train_predicted+.01) :3f}\")\n",
    "print(f\"orange: {mean_differences(y_train, y_cross) :3f}\")\n",
    "print(f\"red: {mean_differences(y_train, y_over)}\")\n",
    "\n",
    "plt.scatter(X_train, y_train)\n",
    "plt.plot(X_train, y_train_predicted+0.01, color='green')\n",
    "plt.plot(X_train, y_cross, color='orange')\n",
    "plt.plot(X_train, y_over, color='red')\n",
    "# plt.ylim(0, 260)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очевидно, что выбор прямой с минимальной средней разностью будет не лучшим решением, ведь в нашем случае это красная прямая. Это произошло от того, что все точки находятся под красной кривой и их разности отрицательные. Так, суммируя разности мы получили отрицательное число с большим абсолютным значением.\n",
    "\n",
    "Заметим, что средняя разность оранжевой прямой, хотя она совсем не выражает зависимость в данных, довольно близка к нулю, как и средняя разность зеленой прямой. Посмотрим, какие значения принимают разности для оранжевой прямой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((y_train - y_cross)[:5])      # первые 5 образцов\n",
    "print((y_train - y_cross)[-5:])     # последние 5 образцов"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часть точек оказалось под оранжевой прямой и их разности отрицательны, а другая часть оказалась над прямой, их разности положительны. И несмотря на то, что, абсолютные значения этих разностей весьма велики, при суммировании, они компенсируют друг друга и итоговое значение средней разности оказывается близким к нулю. \n",
    "\n",
    "Если бы мы суммировали не сами разности, а их абсолютные значения, то такой компенсации не получилось бы, и оранжевая прямая дала бы большое значение разности. Обозначим среднее от модулей разностей буквой $E_1$\n",
    "$$\n",
    "E_1 = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat y_i| = \\frac{1}{n}\\left(|y_1 - \\hat y_1| + \\dots + |y_n - \\hat y_n| \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_abs_differences(y1, y2):\n",
    "    return np.sum(np.abs(y1 - y2)) / len(y1)\n",
    "\n",
    "print(\"mean abs differences\")\n",
    "print(f\"green: {mean_abs_differences(y_train, y_train_predicted+.01) :3f}\")\n",
    "print(f\"orange: {mean_abs_differences(y_train, y_cross) :3f}\")\n",
    "print(f\"red: {mean_abs_differences(y_train, y_over)}\")\n",
    "\n",
    "plt.scatter(X_train, y_train)\n",
    "plt.plot(X_train, y_train_predicted+0.01, color='green')\n",
    "plt.plot(X_train, y_cross, color='orange')\n",
    "plt.plot(X_train, y_over, color='red')\n",
    "# plt.ylim(0, 260)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Минимальное значение функции $E_1$ достигается для зеленой прямой. В идеальном случае все точки лежали бы на прямой, и функция $E_1$ приняла бы значение 0. Это минимальное допустимое значение, так как сумма неотрицательных чисел не может быть меньше нуля. Чем больше точек отклоняются от прогнозируемых значений и чем сильнее они отклоняются, тем больше будет $E_1$. Поэтому можно сказать, что функция $E_1$ характеризует ошибочность модели на данных. Чем меньше будет $E_1$, тем меньше ее ошибочность, и, соответственно, лучше результаты. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Метод наименьших квадратов\n",
    "Метод наименьших квадратов представляет собой алгоритм машинного обучения, который позволяет решать задачу линейной регрессии. В этом методе параметры модели подгоняются так, чтобы среднее значение квадратов разностей $y_i - \\hat y_i$ оказалось минимальным.\n",
    "$$\n",
    "E_2 = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat y_i)^2 = \\frac{1}{n}\\left[(y_1 - \\hat y_1)^2 + \\dots + (y_n - \\hat y_n)^2 \\right]\n",
    "$$\n",
    "В случае одномерной регрессии минимальное значение функции $E_2$ достигается при следующих значениях параметров $a$ и $b$:\n",
    "$$\\hat a = \\overline y - \\hat b \\overline x$$\n",
    "$$\\hat b = \\frac{ \\sum_{i=1}^n (x_i - \\overline x)(y_i - \\overline y)}{\\sum_{i=1}^n (x_i - \\overline x)^2}\n",
    "$$\n",
    "где $ \\overline x$ и  $\\overline y$ - это средние значения.\n",
    "\n",
    "Именно таким образом вычисляются оптимальные параметры модели класса `LinearRegression` при вызове метода `fit()`. Получить коэффициенты $a$ и $b$ из модели можно через свойства `intercept_` и `coef_` соответственно:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = model.intercept_\n",
    "b = model.coef_\n",
    "print(f'{a = }, {b = }')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При вызове метода `predict()` модель вычисляет прогнозируемое значение для каждого $i$-го образца из `X_test` по формуле $\\hat y_i = a + bx_i$. Убедимся в этом самостоятельно вычислив значения $\\hat y_i$ воспользовавшись полученными из модели параметрами `a` и `b`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = a + b * X_train\n",
    "# y = model.predict(X_train.reshape(-1, 1))\n",
    "\n",
    "plt.scatter(X_train, y_train, c='grey', s=15)\n",
    "plt.scatter(X_test, y_test, c='green')\n",
    "plt.plot(X_train, y)\n",
    "# plt.plot(X_train, y_train_predicted)\n",
    "plt.ylim(0, 270)\n",
    "plt.xlabel('temperature')\n",
    "plt.ylabel('ice_cream_sales')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Разбиение данных на обучающую и тестовую выборки "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv('ice_cream_selling_train.csv')\n",
    "\n",
    "train_set, test_set = train_test_split(data, test_size=0.2, random_state=42)\n",
    "print(type(train_set))  # типы данных DataFrame\n",
    "print(len(train_set), len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
